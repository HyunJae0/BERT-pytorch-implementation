{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6373d03a8b026bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spinningup.openai.com/en/latest/spinningup/rl_intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115aef1c6e93328",
   "metadata": {},
   "source": [
    "## Key Concepts and Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34194b71e26e234",
   "metadata": {},
   "source": [
    "RL은 agent와 agent가 시행착오를 통해 학습하는 방법. action에 대해 reward를 주거나 punishment를 가함으로써, agent는 미래에 그 action을 반복하거나 포기할 가능성(즉, 다른 action을 취할 가능성)을 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137bc4711a75047",
   "metadata": {},
   "source": [
    "![image.png](attachment:3f91c125-d78d-459c-b978-b521849a6d16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7885e8a8f109649",
   "metadata": {},
   "source": [
    "RL의 주요 구성 요소는 \"agent\"와 \"environment\"이다.\n",
    "- environment: agent가 상호작용하며 학습하는 world\n",
    "\n",
    "agent와 environment의 interaction loop는 다음과 같다.\n",
    "- agent는 매 상호작용 단계에서 world의 state를 observation한다.\n",
    "- 그다음 agent는 어떤 action을 취할지 결정한다.\n",
    "- agent가 action을 취할 때 environment는 변하며, environment 스스로 변하기도 한다.\n",
    "- agent는 environment로부터 reward signal을 받는다. 이는 어떤 action을 취했을 때, 그 결과가 현재 world의 state가 얼마나 좋은지 나쁜지를 알려주는 숫자이다.\n",
    "\n",
    "agent의 목표는 누적 보상을 최대화하는 것이다. RL methods은 agent가 이 목표를 달성하기 위한 behaviors을 학습할 수 있게 하는 방법들이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a08f3ad4c86368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f983cc8789f0fdc5",
   "metadata": {},
   "source": [
    "### 1. States and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04d6b3f247e830",
   "metadata": {},
   "source": [
    "state(s)는 world의 state에 대한 complete description으로, world에 대한 모든 정보가 있다.\n",
    "\n",
    "observation(o)는 state에 대한 partial description으로, 일부 정보가 생략될 수 있다.\n",
    "\n",
    "딥러닝 RL에서 states과 observations은 거의 항상 실수형 벡터(real-valued vector), 행렬(matrix), 또는 고차원 텐서(tensor)로 표현된다.\n",
    "\n",
    "agent가 environment의 complete state를 관측할 수 있으면 그 environment를 \"fully observed\"되었다고 말하며, 부분적인 관측만 가능하면 \"partially observed\"되었다고 말한다.\n",
    "- fully observed의 경우 agent가 해당 environment와 관련된, 또는 action에 의해서 영향을 받는 변수들을 모두 관찰할 수 있는 것이다.\n",
    "- 예를 들어, 모든 패가 공개되어 진행되는 게임의 경우는 모든 states이 공개된 fully observed environment이며, 상대방에게 보여지는 패도 있고, 보여지지 않는 패도 있는 게임은 partially observed environment이다.\n",
    "\n",
    "cf) RL notation에서는 observation o를 쓰는 것이 더 적절함에도 state의 symbol인 s를 쓰는 경우가 많다. 특히 agent가 action a를 결정할 때, 실제로는 agent가 s에 접근할 수 없기 때문에 o를 보고 결정하지만, 표기상으로는 action이 s에 조건부(즉, (a|s))라고 쓰는 경우가 많다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ad41e95be5f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c8a34f508892c7a",
   "metadata": {},
   "source": [
    "### 2. Action Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74160bfe190b60b2",
   "metadata": {},
   "source": [
    "environment에 따라 허용되는 action의 종류가 다르다.\n",
    "\n",
    "주어진 environment에서 모든 유효한 actions의 집합을 \"action space\"라고 하며, action space에는 discrete action space와 continuous action space가 있다.\n",
    "- (1) discrete action space: 아타리 게임이나 바둑처럼 agent가 할 수 있는 동작들(즉, actions)의 수가 유한한 경우\n",
    "- (2) continuous action space: 물리적 세계에서 로봇을 제어하는 것처럼 actions이 real-valued vectors로 정의되는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bc7daeb2f43bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cac556ef25499c1",
   "metadata": {},
   "source": [
    "### 3. Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f99ebc9ddf1f4",
   "metadata": {},
   "source": [
    "policy는 agent가 어떤 action을 취할지 결정하기 위해 사용하는 규칙이다.\n",
    "\n",
    "#### __3.1 Deterministic Policy__\n",
    "policy는 deterministic할 수 있다. 이는 특정 state에서 항상 동일한 action을 하는 것이다. 즉, 특정 state에서는 취할 action이 정해져 있다. 어떤 state $s_t$가 주어지면, 항상 동일한 action $a_t$가 나온다. 이를 보통 $\\mu$로 표기한다: $a_t = \\mu(s_t)$\n",
    "\n",
    "어떤 state $s_t$가 주어지면, 항상 동일한 action $a_t$가 나오기 때문에, $a_t$를 선택할 확률은 1이고, 나머지 actions의 확률은 0이 된다.\n",
    "- 이미 선택해야 할 action $a_t$이 정해져 있으므로, 해당 $a_t$를 선택할 확률은 무조건 1일 수 밖에 없다. 그러므로 나머지 actions의 확률들은 자연스럽게 모두 0이다.\n",
    "\n",
    "#### __3.2 Stochastic Policy__\n",
    "그리고 stochastic할 수도 있다. 이는 각 state에서 취할 action이 정해져있지 않고 확률적임을 의미한다. 즉, state $s_t$에 대해 action $a_t$를 확률적으로 선택한다. 그러므로, 동일한 state에서도 서로 다른 action이 선택될 수 있으며, 이는 확률분포에 따라 결정된다.\n",
    "- 예를 들어, 어떤 state에서 오른쪽으로 갈 확률이 40%, 왼쪽으로 갈 확률이 30%, 직진할 확률이 30%라면 agent는 세 가지 actions 중 하나를 확률적으로 선택한다.\n",
    "\n",
    "즉, stochastic policy는 특정 state에서 action의 확률분포를 따른다. 보통 $\\pi$로 표기한다: $a_t \\sim \\pi(\\cdot|s_t)$\n",
    "\n",
    "\n",
    "이러한 policy는 본질적으로 agent의 뇌와 같아서, 종종 \"agent\"라는 단어를 \"policy\"와 혼용하여 사용하기도 한다. (예: \"policy(=agent)가 reward를 최대화하려고 한다\")\n",
    "\n",
    "딥러닝 RL에서는 parameterized policy를 다룬다: policy의 output이 파라미터 집합(예: 신경망의 가중치와 편향)에 의존한다. 즉, 최적화 알고리즘을 통해 이 파라미터를 조정하여 behavior를 변경할 수 있는 policy이다.\n",
    "- 파라미터는 주로  $\\theta$나 $\\phi$로 표기한다.\n",
    "$$a_t = \\mu_\\theta(s_t)$$ $$a_t \\sim \\pi_\\theta(\\cdot|s_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48cfc54c7de28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Deterministic Policy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(seed=42)\n",
    "obs_dim, act_dim = 3, 2 # 하나의 관측은 길이(차원) 3짜리 벡터, 하나의 행동은 길이 2짜리 벡터\n",
    "batch_size = 4 # 관측 4개\n",
    "\n",
    "pi_net = nn.Sequential(\n",
    "    nn.Linear(obs_dim, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, act_dim),\n",
    ")\n",
    "\"\"\"\n",
    "pi_net은 64 크기의 hidden layer 2개와 tanh를 activation function으로 사용하는 MLP이다.\n",
    "입력은 observation이고 output은 action이다.\n",
    "observation obs가 관측 배치를 담은 텐서(또는 배열)라면, pi_net을 통해 각 관측에 대한 행동을 얻을 수 있다.\n",
    "pi_net은 확률분포로부터 샘플링하지 않는 deterministic policy이다. state -> action을 바로 계산하고, 그 결과를 그대로 사용하는 구조이다.\n",
    "\"\"\"\n",
    "\n",
    "obs = torch.Tensor([\n",
    "    [1.0,  0.5, -0.2],\n",
    "    [0.3, -1.2,  2.0],\n",
    "    [-0.7,  0.0,  0.1],\n",
    "    [2.0,  1.5, -3.0],\n",
    "])\n",
    "\n",
    "actions = pi_net(obs)\n",
    "# print(obs.shape) # (4, 3) -> 관측 4개, 각 관측은 3차원 벡터\n",
    "# print(actions.shape) # (4, 2) ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de2fc55a96d44c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions\n",
    "# 첫 번째 관측 [1.0,  0.5, -0.2] -> 행동 [0.053951, 0.038734]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14799362afd5db24",
   "metadata": {},
   "source": [
    "#### 3.1 Stochastic Policy의 종류\n",
    "\n",
    "딥러닝 RL에서 가장 일반적인 두 가지 stochastic policies은 \"categorical policies\"과 \"diagonal Gaussian policies\"이다.\n",
    "\n",
    "categorical policies은 discrete action spaces에서 사용되고, diagonal Gaussian policies은 continuous action spaces에서 사용된다.\n",
    "\n",
    "stochastic policies을 사용하고 학습시킬 때, 중요한 두 가지 계산은 다음과 같다.\n",
    "- (1) policy로부터 actions을 sampling하기\n",
    "- (2) 특정 actions의 log likelihoods 계산하기: $\\log \\pi_\\theta(a|s)$\n",
    "\n",
    "##### __3.1.1 Categorical Policies__\n",
    "categorical policy는 discrete actions에 대한 classifier와 같다.\n",
    "\n",
    "categorical policy를 위한 neural network는 classifier를 구축하는 방식과 동일하게 구축한다: input은 observation이고, 그 뒤에 여러 개의 layers(input 유형에 따라 convolutional 또는 densely-connected layer일 수 있음 )이 이어진다. 마지막으로 각 action에 대한 logits을 출력하는 linear layer가 있고, 그 뒤에 softmax를 사용하여 logits을 probabilities로 변환한다.\n",
    "\n",
    "input(observation) -> [convolutional or densely-connected layers] -> [linear layer] => logits -> [softmax] => action에 대한 확률\n",
    "\n",
    "__Sampling.__ 각 action에 대한 probabilities이 주어지면, sampling tools이 built-in된 프레임워크(PyTorch, Tensorflow 등)를 사용해서 샘플링한다.\n",
    "- 예:  torch.multinomial, tf.distributions.Categorical, or tf.multinomial\n",
    "\n",
    "__Log-Likelihood.__ 확률 벡터를 $P_\\theta(s)$라고 할 때, 이는 actions의 개수만큼 항목을 가진 벡터이다. 그래서 actions을 벡터의 인덱스로 취급할 수 있다.\n",
    "\n",
    "그러므로, 특정 action a에 대한 log likelihood는 action a를 벡터의 인덱스로 사용하여 다음과 같이 계산할 수 있다. $$\\log \\pi_\\theta(a|s) = \\log [P_\\theta(s)]_a$$\n",
    "\n",
    "\n",
    "##### __3.1.2 Diagonal Gaussian Policies__\n",
    "multivariate Gaussian distribution(다변량 정규 분포)는 평균 벡터 $\\mu$와 공분산 행렬 $\\Sigma$로 설명된다. diagonal Gaussian distribution은 special case로 공분산 행렬이 대각선 성분(주대각성분)만 갖는, 즉 주대각성분을 제외한 행렬 내 모든 원소가 0인 값을 갖는다.\n",
    "\n",
    "diagonal Gaussian policy는 observations을 mean actions $\\mu_\\theta(s)$로 매핑하는 neural network를 항상 가진다.\n",
    "\n",
    "공분산 행렬을 표현하는 방법에는 두 가지가 있다.\n",
    "- (1) state의 function이 아닌(즉, state와 무관한) 하나의 로그 표준편차 벡터 $\\log \\sigma$를 사용하는 것이다. 즉, $\\log \\sigma$는 독립적인 파라미터이다. TRPO, PPO 구현에서 이 방식을 주로 사용한다.\n",
    "- (2) states을 로그 표준편차 $\\log \\sigma_{\\theta}(s)$로 매핑하는 neural network를 사용하는 것이다.\n",
    "\n",
    "로그 표준편차를 사용하는 이유는 다음과 같다.\n",
    "- 표준편차 $\\sigma$는 0보다 크거나 같다(즉, 음수가 될 수 없다)는 제약이 있지만, 로그 표준편차는 $-\\infty$에서 $\\infty$ 범위의 어떤 값이든 가질 수 있어 학습이 더 쉽다. ($\\sigma$는 음수가 될 수 없다와 같은 제약 조건을 적용하지 않아도 되므로 파라미터 학습이 더 쉽다)\n",
    "- 표준편차는 로그 표준편차를 지수화하면 바로 얻을 수 있다. 그러므로, 로그 표준편차로 표현해도 손실되는 것은 없다.\n",
    "\n",
    "__Sampling.__ mean action $\\mu_\\theta(s)$와 표준편차 $\\sigma_\\theta(s)$, 그리고 spherical Gaussian에서 추출한 noise vector z가 주어졌을 때($z∼\\mathcal{N}(0,I)$), action sample은 다음과 같이 계산된다.  $$a = \\mu_\\theta(s) + \\sigma_\\theta(s) \\odot z$$\n",
    "- 여기서 $\\odot$은 두 벡터의 elementwise product를 의미한다.\n",
    "- standard frameworks에는 noise vectors을 생성하는 내장 기능이 있다: torch.normal or tf.random_normal 또는 torch.distributions.Normal or tf.distributions.Normal과 같은 분포 객체를 생성하여 sample을 생성할 수도 있다. 후자의 장점은 해당 객체가 log-likelihood도 계산해 준다는 것이다.\n",
    "\n",
    "__Log-Likelihood.__  평균이 $\\mu = \\mu_{\\theta}(s)$이고 표준편차가 $\\sigma = \\sigma_{\\theta}(s)$인 대각 가우시안 분포에서 $k$차원의 action a에 대한 log-likelihood는 다음과 같이 계산된다. $$\\log \\pi_\\theta(a|s) = -\\frac{1}{2}\\left(\\sum_{i=1}^k \\left(\\frac{(a_i - \\mu_i)^2}{\\sigma_i^2} + 2 \\log \\sigma_i \\right) + k \\log 2\\pi \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8d93bc88a92de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f570826df223d465",
   "metadata": {},
   "source": [
    "### 4. Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7e530a06709b",
   "metadata": {},
   "source": [
    "trajectory $\\tau$는 world에서 일어나는 states과 actions의 sequence이다: $\\tau = (s_0, a_0, s_1, a_1, ...)$\n",
    "- trajectory는 episode 또는 rollout이라고도 불린다.\n",
    "\n",
    "world의 가장 첫 번째 상태 $s_0$는 시작 상태 분포(start-state distribution)에서 무작위로 샘플링된다.\n",
    "\n",
    "__state transitions__ time $t$에서의 state $s_t$와 $t+1$의 state $s_{t+1}$ 사이에서 world에 일어나는 일은 environment의 natural laws에 의해 결정되며, 가장 최근의 action $a_t$에만 의존한다. (즉, next state $s_{t+1}$은 오직 current state $s_t$와 action $a_t$에 의해 결정된다) 이는 deterministic일 수도 있고, stochastic일 수도 있다.\n",
    "- deterministic:  $s_{t+1} = f(s_t, a_t)$\n",
    "- stochastic: $s_{t+1} \\sim P(\\cdot|s_t, a_t)$\n",
    "\n",
    "이러한 state transition $(s_{t+1} | s_t, a_t)$은 agent가 제어할 수 없는 부분이다. agent가 제어할 수 있는 유일한 부분은 $policy(a_t | s_t)$이다. 즉, action은 agent의 policy에 따라 나온다. 선택한 action의 결과로 environment가 어떻게 반응할지는 agent의 통제 밖이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29c7864c70b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc896414df55c7ad",
   "metadata": {},
   "source": [
    "### 5. Reward and Return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f6842f5b187f2",
   "metadata": {},
   "source": [
    "reward function $R$은 current state와 action, 그리고 next state에 의존한다: $r_t = R(s_t, a_t, s_{t+1})$\n",
    "- 종종  $r_t = R(s_t)$ 또는 $r_t = R(s_t, a_t)$로 단순화하여 사용한다.\n",
    "\n",
    "agent의 목표는 trajectory(episode) $\\tau$ 전체에 걸쳐 누적 보상(return)을 최대화하는 것이지만, return은 horizon과 discounting에 따라 의미가 달라질 수 있다.\n",
    "- time $t$에서 얻은 단일 시점의 보상을 reward라 하고, 이러한 rewards을 시간에 따라 누적한 값을 return이라고 한다.\n",
    "\n",
    "trajectory 전체에 걸친 누적 보상(즉, 시간에 따라 누적된 보상)을 $R(\\tau)$라고 했을 때,\n",
    "- (1) finite-horizon undiscounted return: fixed timesteps 동안 얻은 rewards의 단순 합계이다. $$R(\\tau) = \\sum_{t=0}^T r_t$$\n",
    "- (2) infinite-horizon discounted return: agent가 얻은 모든 rewards의 합계이지만, 미래에 얻을수록 discount factor $\\gamma \\in (0, 1)$ 만큼 깎아서 더한다. $$R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t$$\n",
    "- discount factor를 적용하지 않고 모든 rewards을 받는 게 더 좋아 보이지만, discount factor를 사용하는 이유에는 직관적인 이유와 수학적인 이유가 모두 들어간다.\n",
    "- discount factor는 시간이 지날수록 reward의 중요도를 낮춘다. 이는 직관적으로 먼 미래의 불확실성(먼 미래로 갈수록 environment가 바뀔 수도 있고, 먼 미래의 state까지 도달하지 못할 수도 있다. 그래서 미래의 reward는 unreliable하다.)을 반영하기 위한 것이다. 이를 통해 당장 가까운 미래의(즉, 현재 시점과 가까운) reward를 더 중요하게 여기게 된다.\n",
    "- 수학적으로는, 무한한 rewards의 합은 발산할 수 있지만, discount factor가 있으면 합이 유한한 값으로 수렴하게 된다. (수치적 안정성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ee6167ecf3f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f03ecfddbdf8dead",
   "metadata": {},
   "source": [
    "### 6. The RL Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289673e38554b0f",
   "metadata": {},
   "source": [
    "return의 척도를 무엇으로 선택하든(finite-horizon undiscounted return vs. infinite-horizon discounted return), 어떤 policy를 선택하든(deterministic policy vs. stochastic policy), RL의 목표는 __agent가 policy에 따라 행동할 때 \"expected return\"을 최대화하는 policy를 선택하는 것이다.__\n",
    "\n",
    "expected return에 대해 논하려면 trajectory에 대한 확률분포를 이야기해야 한다. environment transitions와 policy가 모두 확률적(stochastic)이라고 가정할 때, $T$-step trajectory의 확률은 다음과 같다. $$P(\\tau|\\pi) = \\rho_0(s_0) \\prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t)\\pi(a_t|s_t)$$\n",
    "- trajectory $\\tau$가 발생할 확률은 시작 상태일 확률 $\\rho_0(s_0)$에 시작 상태에서 어떤 action을 선택할 확률과 그 action으로 next state가 될 확률을 계속 곱한 값이다.\n",
    "- 즉, \"시작 상태일 확률 × (그 상태에서 행동할 확률 × 그 행동으로 다음 상태가 될 확률)..\"을 계속 곱한 값이다.\n",
    "\n",
    "그리고 어떤 return 척도를 사용하든, expected return $J(\\pi)$는 다음과 같다.$$J(\\pi) = \\int_{\\tau} P(\\tau|\\pi)R(\\tau) = \\underset{\\tau \\sim \\pi}{\\text{E}}[R(\\tau)]$$\n",
    "- 미래는 불확실하므로, 운 좋게 가장 점수를 많이 받은 경우가 아니라 \"평균적으로 점수를 가장 많이 얻는 방법\"을 찾아야 한다. 이것이 $J(\\pi)$를 최대화하는 이유이다.\n",
    "\n",
    "그러므로, RL의 최적화 문제는 다음과 같이 표현할 수 있다.  $$\\pi^* = \\arg \\max_{\\pi} J(\\pi)$$\n",
    "- 여기서 $\\pi^*$는 optimal policy이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1c98445cf39aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b93fb97f35b64ebf",
   "metadata": {},
   "source": [
    "### 7. Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6af982fffdc439",
   "metadata": {},
   "source": [
    "state 또는 state-action pair의 value를 아는 것은 유용하다. \"지금 이 선택이 장기적으로 얼마나 좋은지\"를 수치로 비교할 수 있기 때문이다.\n",
    "\n",
    "value는 state 또는 state-action pair에서 시작하여 특정 policy를 쭉 따라 영원히 행동했을 때 얻을 수 있는 expected return을 의미한다.\n",
    "\n",
    "value function은 거의 모든 RL algorithm에서 직간접적으로 사용되며, 4가지 주요 함수가 있다.\n",
    "- (1) On-Policy Value Function $V^{\\pi}(s)$: state $s$에서 시작해서 policy $\\pi$를 따를 때의 expected return $$\n",
    "V^{\\pi}(s) = \\underset{\\tau \\sim \\pi}{\\text{E}} [R(\\tau) \\mid s_0 = s]\n",
    "$$\n",
    "- state $s$가 좋은지 나쁜지 평가하는 데 사용할 수 있다.\n",
    "- (2) On-Policy Action-Value Function $Q^{\\pi}(s, a)$: state $s$에서 임의의 action $a$를 선택한 다음($\\pi$에서 나온 것이 아닐 수도 있음), 그 이후에는 $\\pi$를 쭉 따랐을 때의 expected return $$\n",
    "Q^{\\pi}(s, a) = \\underset{\\tau \\sim \\pi}{\\text{E}} [R(\\tau) \\mid s_0 = s, a_0 = a]\n",
    "$$\n",
    "- (3) Optimal Value Function $V^*(s)$: state $s$에서 시작해서 항상 optimal policy에 따라 행동했을 때의 expected return $$\n",
    "V^*(s) = \\max_{\\pi} \\underset{\\tau \\sim \\pi}{\\text{E}} [R(\\tau) \\mid s_0 = s]\n",
    "$$\n",
    "- (4) Optimal Action-Value Function $Q^*(s, a)$: state $s$에서 action $a$를 취하고, 그 이후에는 optimal policy를 쭉 따랐을 때의 expected return $$\n",
    "Q^*(s, a) = \\max_{\\pi} \\underset{\\tau \\sim \\pi}{\\text{E}} [R(\\tau) \\mid s_0 = s, a_0 = a]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "__value function과 action-value function 사이의 관계__\n",
    "- (1) $V^{\\pi}(s) = \\underset{a \\sim \\pi}{\\text{E}} [Q^{\\pi}(s, a)]$: 특정 policy $\\pi$를 따를 때, current state $s$의 value($V$)는, 그 state에서 policy $\\pi$가 선택할 모든 actions의 value($Q$)의 평균(기댓값)과 같다.\n",
    "- 어떤 하나의 state $s$가 있을 때, 이 $s$에서 $\\pi(a|s)$를 통해 선택될 수 있는 action $a$가 여러 개 있다.\n",
    "- 특정 $a$를 선택했을 때의 expected return은 $Q^{\\pi}(s, a)$이다.\n",
    "- (discrete action space를 가정) 그러므로, state $s$ 자체의 가치는 \"$s$에서 각 action을 선택할 확률 x 그 action의 가치\"를 모든 $a$에 대해 더한 것과 같다: $$V^{\\pi}(s) = \\sum_{a} \\pi(a|s) \\cdot Q^{\\pi}(s, a)$$\n",
    "- 확률에서, 어떤 이산확률변수가 가질 수 있는 값들에 그 값이 발생할 확률을 곱하여 모두 더한 값은 기댓값이다. 즉, 이 합계($\\sum$)가 바로 확률에서의 기댓값의 정의이다.\n",
    "- (2) $V^*(s) = \\max_a Q^*(s, a)$: optimal policy를 따를 때, state $s$의 optimal value(V)는 그 state에서 선택할 수 있는 모든 action 중 가장 좋은 action의 value(Q)와 같다.\n",
    "- optimal policy는 가능한 모든 policy들 중 가장 높은 expected return을 내는 policy이다. 즉, $s$에서 가장 value가 높은 action을 반드시 선택한다는 것이다.\n",
    "- state $s$에서 선택 가능한 actions이 A(value: 10), B(value: 50), C(value: 20)가 있다고 했을 때, optimal policy라면 당연히 value가 가장 높은 B를 선택할 것이다.\n",
    "- 그러므로 당연히 optimal value는 50점이다. optimal이라면 무조건 B이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62108a472dcdbcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f179cfa65b02ef53",
   "metadata": {},
   "source": [
    "### 8. The Optimal Q-Function and the Optimal Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ccf1fdba3675b9",
   "metadata": {},
   "source": [
    "optimal action-value function $Q^(s, a)$와 optimal policy에 의해 선택되는 action 사이에는 중요한 연결 관계가 있다.\n",
    "\n",
    "정의에 따르면, $Q^(s, a)$는 state $s$에서 시작하여 (임의의) action $a)를 한 번 취한 뒤, 그 이후에는 영원히 optimal policy를 따라 행동한다고 가정했을 때 얻을 수 있는 expected return을 나타낸다.\n",
    "\n",
    "$s$에서의 optimal policy는, $s$에서 시작했을 때의 expected return을 최대화하는 action을 선택할 것이다.\n",
    "\n",
    "그러므로, 만약 $Q^*$를 이미 알고 있다면, 다음과 같이 optimal action $a^*(s)$를 직접 구할 수 있다. $$a^*(s) = \\arg \\max_a Q^*(s, a)$$\n",
    "- $Q^*(s, a)$ 자체가 이미 \"state $s$에서 이 action $a$를 선택했을 때의 최선의 미래\"를 담고 있으므로, state $s$에서 $Q^*(s, a)$가 가장 높은 action(즉, $\\arg \\max_a Q^*(s, a)$)이 곧 가장 좋은 action이 된다. 그냥 점수가 가장 높은 action을 고르기만 하면 그것이 곧 최적의 선택이기 때문이다.\n",
    "\n",
    "참고로, $Q^*(s, a)$를 최대화하는 여러 개의 actions이 존재할 수 있다. 이 경우, 그 actions은 모두 optimal actions이기 때문에, optimal policy는 그중 아무거나 무작위로 선택할 수도 있다. 그리고 여러 optimal actions 중 하나를 결정적으로(deterministically) 선택하는 optimal policy는 항상 존재한다.\n",
    "- 예를 들어 왼쪽 길을 갈 확률이 50%, 오른쪽 길을 갈 확률도 50%라고 하자. 이때, stochastic optimal policy에서는 왼쪽 길을 가도, 오른쪽 길을 가도 상관없다.  둘 다 최적이기 때문이다.\n",
    "- deterministic optimal policy에서는 이렇게 최적의 actions이 여러 개여도, 그중 하나를 딱 선택해서 행동하게 하는 optimal policy를 만들 수 있다. 이 예에서, \"점수가 같으면 무조건 왼쪽으로 간다\"와 같은 규칙을 정하면 되기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e944842da7eb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "988988618a3aa3ef",
   "metadata": {},
   "source": [
    "### 9. Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27328c1a9774df46",
   "metadata": {},
   "source": [
    "4가지 value functions($V^\\pi, Q^\\pi, V^*, Q^*$) 모두 벨만 방정식을 따른다. 벨만 방정식의 기본 아이디어: \"시작점의 value는 그곳에서 기대하는 reward에, 바로 다음에 도착할 곳의 value를 더한 것과 같다\"\n",
    "\n",
    "on-policy value functions에 대한 벨만 방정식은 다음과 같다. $$V^{\\pi}(s) = \\underset{\\substack{a \\sim \\pi \\\\ s' \\sim P}}{\\text{E}} [r(s, a) + \\gamma V^{\\pi}(s')] \\quad \\text{}$$$$Q^{\\pi}(s, a) = \\underset{s' \\sim P}{\\text{E}} [r(s, a) + \\gamma \\underset{a' \\sim \\pi}{\\text{E}}[Q^{\\pi}(s', a')]] \\quad \\text{}$$\n",
    "- 여기서 $s' \\sim P$는 $s' \\sim P(\\cdot|s, a)$의 줄임말로, next state $s'$이 environment의 transition rules에 따라 샘플링됨을 의미합니다.\n",
    "- $a \\sim \\pi$는 $a \\sim \\pi(\\cdot|s)$의 줄임말이며, $a' \\sim \\pi$는 $a' \\sim \\pi(\\cdot|s')$의 줄임말이다.\n",
    "\n",
    "optimal value functions에 대한 벨만 방정식은 다음과 같다. $$V^*(s) = \\max_a \\underset{s' \\sim P}{\\text{E}} [r(s, a) + \\gamma V^*(s')] \\quad \\text{}$$$$Q^*(s, a) = \\underset{s' \\sim P}{\\text{E}} [r(s, a) + \\gamma \\max_{a'} Q^*(s', a')] \\quad \\text{}$$\n",
    "\n",
    "on-policy value functions과 the optimal value functions의 결정적인 차이점은 actions에 대한 max 연산자의 유무이다.\n",
    "\n",
    "max가 포함되었다는 것은 agent가 최적으로 행동하려면 가장 높은 value로 이어지는 action을 선택해야 한다는 사실을 나타낸다.\n",
    "\n",
    "__벨만 방정식이 중요한 이유는, value를 계산할 때 필요한 연산을 줄여주기 때문이다.__\n",
    "- 예를 들어, value function $V(s)$가 현재 state에서 시작해서 얻을 수 있는 모든 미래의 rewards의 합의 기댓값이라고 하자: $V(s) = E[r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\dots]$\n",
    "- 이 정의대로 $V(s)$를 계산하려면 두 가지 문제가 발생한다.\n",
    "- (1) trajectory(episode)가 끝나지 않는 environment라면, $V(s)$를 구하기 위해 무한히 많은 미래의 rewards을 더해야 한다.\n",
    "- (2) current state에서 선택할 수 있는 actions이 $A$개이고, next state로 가능한 전이의 수가 $S$개($A$가지 action을 선택할 수 있고, 각 action마다 여러 개의 next state로 이동할 수 있다)라고 하자. trajectory의 길이가 길어질수록 모든 가능한 경로를 열거하여 value를 계산하는 것은 현실적으로 불가능하다.\n",
    "- 벨만 방정식은 이 식을 재귀적으로 바꾼다: $$V(s) = E[r_t + \\gamma V(s_{t+1})]$$\n",
    "- 기존 정의에 있던 무한한 뒷부분($r_{t+1} + \\gamma r_{t+2} + \\dots$)을 $V(s_{t+1})$이라는 단 하나의 값으로 대체했다. 이미 계산한(혹은 추정한) next state의 value 값을 그대로 가져와서 사용한다. 이를 통해 다음 상태의 value만으로 현재의 value를 판단할 수 있다.\n",
    "\n",
    "참고로 RL 문헌에서 벨만 백업(Bellman backup)이라는 용어가 등장한다. 어떤 state(or state-action pair)에 대한 벨만 백업은 벨만 방정식의 우변(reward + next value)을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85515f1e753f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad15bfdf3577f45",
   "metadata": {},
   "source": [
    "### 10. Advantage Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe96ec235de4564",
   "metadata": {},
   "source": [
    "RL에서는 어떤 action이 절대적으로 얼마나 좋은지를 측정하는 대신, 단지 해당 action이 다른 actions에 비해 \"평균적으로 얼마나 더 나은지\"만 알면 될 때가 있다. 즉, action의 상대적 이점(relative advantage)을 알고 싶은 것이다. 이는 advantage function으로 계산할 수 있다.\n",
    "\n",
    "advantage function $A^{\\pi}(s, a)$는, state $s$에서 특정 action $a$를 취하는 것이 $\\pi(\\cdot|s)$에 따라 무작위로 action을 선택하는 것보다 얼마나 더 나은지를 나타낸다.\n",
    "\n",
    "advantage function은 다음과 같이 정의된다.\n",
    "$$A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)$$\n",
    "- $Q^{\\pi}(s, a)$는 특정 action $a$를 했을 때의 expected return이다. 이 정보만으로는 이 action이 좋은 건지 나쁜 건지 판단하기 어렵다. 만약, 동일한 $s$에서 다른 actions의 평균 점수가 더 높다면, 이 action은 나쁜 선택이기 때문이다.\n",
    "- 그래서 $Q^{\\pi}(s, a)$에서 policy $\\pi$에 따라 평균적으로 기대되는 return인 $V^{\\pi}(s)$를 빼줌으로써, 특정 $s$에서의 어떤 action $a$가 다른 actions에 비해 상대적으로 얼마나 좋은가를 계산한다.\n",
    "-  $A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)$를 계산했을 때,\n",
    "- (1) $A^{\\pi}(s, a) > 0$이라면, 이 $a$가 평균보다 더 좋다 -> policy gradient 관점에서, 이 $a$를 더 선택하기 위해 $a$의 확률을 증가시키는 방향으로 업데이트\n",
    "- (2) $A^{\\pi}(s, a) < 0$이라면, 이 $a$가 평균보다 더 나쁘다 -> 이 $a$ 말고 다른 action을 선택하도록 업데이트\n",
    "- (3) $A^{\\pi}(s, a) = 0$이라면, 이 $a$가 평균과 동등하다는 의미가 된다. -> (1)과 (2)에서와 같은 적극적인 업데이트가 발생하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f1dde7146c096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
